"""
BigModelçŸ¥è¯†åº“ç®¡ç†å™¨
ä½¿ç”¨æ™ºè°±AIçš„embedding-2æ¨¡å‹å’ŒChromaDBæ„å»ºçŸ¥è¯†åº“
"""

import chromadb
from chromadb.config import Settings
import os
import re
from typing import List, Dict, Any, Optional
import numpy as np
from services.bigmodel_embedding import BigModelEmbedding
from services.bigmodel_embedding_function import BigModelEmbeddingFunction
from core.config import Config

class BigModelKnowledgeBase:
    """ä½¿ç”¨BigModel embeddingçš„çŸ¥è¯†åº“ç®¡ç†å™¨"""
    
    def __init__(self, api_key: str = None, collection_name: str = "engineering_knowledge_bigmodel"):
        """
        åˆå§‹åŒ–çŸ¥è¯†åº“ç®¡ç†å™¨
        
        Args:
            api_key: BigModel APIå¯†é’¥
            collection_name: é›†åˆåç§°
        """
        self.api_key = api_key
        self.collection_name = collection_name
        
        # åˆå§‹åŒ–BigModel embeddingæœåŠ¡
        self.embedding_service = BigModelEmbedding(api_key)
        self.embedding_function = BigModelEmbeddingFunction(api_key)
        
        # åˆå§‹åŒ–ChromaDBå®¢æˆ·ç«¯
        self.client = chromadb.PersistentClient(
            path=Config.CHROMA_PERSIST_DIRECTORY,
            settings=Settings(
                anonymized_telemetry=False,
                allow_reset=True
            )
        )
        
        # åˆ›å»ºæˆ–è·å–é›†åˆ
        self.collection = self._get_or_create_collection()
        
        print(f"âœ… BigModelçŸ¥è¯†åº“ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
        print(f"   é›†åˆåç§°: {self.collection_name}")
        print(f"   æ•°æ®åº“è·¯å¾„: {Config.CHROMA_PERSIST_DIRECTORY}")
    
    def _get_or_create_collection(self):
        """è·å–æˆ–åˆ›å»ºé›†åˆ"""
        try:
            # å°è¯•è·å–ç°æœ‰é›†åˆ
            collection = self.client.get_collection(name=self.collection_name)
            print(f"ğŸ“š ä½¿ç”¨ç°æœ‰é›†åˆ: {self.collection_name}")
        except Exception:
            # åˆ›å»ºæ–°é›†åˆï¼Œä½¿ç”¨è‡ªå®šä¹‰åµŒå…¥å‡½æ•°
            collection = self.client.create_collection(
                name=self.collection_name,
                embedding_function=self.embedding_function,
                metadata={"description": "å·¥ç¨‹ç›‘ç†çŸ¥è¯†åº“ - BigModelç‰ˆ"}
            )
            print(f"ğŸ“š åˆ›å»ºæ–°é›†åˆ: {self.collection_name}")
        
        return collection
    
    def _embedding_function(self, input: List[str]) -> List[List[float]]:
        """
        ChromaDBä½¿ç”¨çš„åµŒå…¥å‡½æ•°
        
        Args:
            input: æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            åµŒå…¥å‘é‡åˆ—è¡¨
        """
        embeddings = self.embedding_service.encode(input)
        return embeddings.tolist()
    
    def add_document(self, content: str, metadata: Dict[str, Any] = None) -> str:
        """
        æ·»åŠ å•ä¸ªæ–‡æ¡£åˆ°çŸ¥è¯†åº“
        
        Args:
            content: æ–‡æ¡£å†…å®¹
            metadata: æ–‡æ¡£å…ƒæ•°æ®
            
        Returns:
            æ–‡æ¡£ID
        """
        # ç”Ÿæˆæ–‡æ¡£ID
        doc_id = f"doc_{hash(content) % 1000000}"
        
        # é»˜è®¤å…ƒæ•°æ®
        if metadata is None:
            metadata = {}
        
        metadata.update({
            "content_length": len(content),
            "type": "document"
        })
        
        # è·å–å‘é‡è¡¨ç¤º
        embedding = self.embedding_service.encode([content])[0].tolist()
        
        # æ·»åŠ åˆ°é›†åˆ
        self.collection.add(
            documents=[content],
            embeddings=[embedding],
            metadatas=[metadata],
            ids=[doc_id]
        )
        
        print(f"âœ… æ·»åŠ æ–‡æ¡£: {doc_id}")
        return doc_id
    
    def add_documents_batch(self, documents: List[str], metadatas: List[Dict[str, Any]] = None) -> List[str]:
        """
        æ‰¹é‡æ·»åŠ æ–‡æ¡£
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            metadatas: å…ƒæ•°æ®åˆ—è¡¨
            
        Returns:
            æ–‡æ¡£IDåˆ—è¡¨
        """
        if not documents:
            return []
        
        # ç”Ÿæˆæ–‡æ¡£ID
        doc_ids = [f"doc_{hash(doc) % 1000000}_{i}" for i, doc in enumerate(documents)]
        
        # å¤„ç†å…ƒæ•°æ®
        if metadatas is None:
            metadatas = [{} for _ in documents]
        
        # ä¸ºæ¯ä¸ªæ–‡æ¡£æ·»åŠ åŸºæœ¬å…ƒæ•°æ®
        for i, metadata in enumerate(metadatas):
            metadata.update({
                "content_length": len(documents[i]),
                "type": "document",
                "batch_index": i
            })
        
        # è·å–å‘é‡è¡¨ç¤º
        print(f"ğŸ”„ æ­£åœ¨è·å– {len(documents)} ä¸ªæ–‡æ¡£çš„å‘é‡è¡¨ç¤º...")
        embeddings = self.embedding_service.encode(documents)
        
        # æ‰¹é‡æ·»åŠ åˆ°é›†åˆ
        self.collection.add(
            documents=documents,
            embeddings=embeddings.tolist(),
            metadatas=metadatas,
            ids=doc_ids
        )
        
        print(f"âœ… æ‰¹é‡æ·»åŠ äº† {len(documents)} ä¸ªæ–‡æ¡£")
        return doc_ids
    
    def search(self, query: str, n_results: int = 5, include_distances: bool = True) -> Dict[str, Any]:
        """
        æœç´¢ç›¸å…³æ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            n_results: è¿”å›ç»“æœæ•°é‡
            include_distances: æ˜¯å¦åŒ…å«è·ç¦»ä¿¡æ¯
            
        Returns:
            æœç´¢ç»“æœ
        """
        # è·å–æŸ¥è¯¢å‘é‡
        query_embedding = self.embedding_service.encode([query])[0].tolist()
        
        # æ‰§è¡Œæœç´¢
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=n_results,
            include=['documents', 'metadatas', 'distances']
        )
        
        # æ ¼å¼åŒ–ç»“æœ
        formatted_results = {
            "query": query,
            "results": []
        }
        
        if results['documents'][0]:  # æ£€æŸ¥æ˜¯å¦æœ‰ç»“æœ
            for i in range(len(results['documents'][0])):
                result_item = {
                    "id": results['ids'][0][i],
                    "content": results['documents'][0][i],
                    "metadata": results['metadatas'][0][i],
                }
                
                if include_distances and 'distances' in results:
                    # ChromaDBè¿”å›çš„æ˜¯è·ç¦»ï¼Œè½¬æ¢ä¸ºç›¸ä¼¼åº¦ï¼ˆ1 - å½’ä¸€åŒ–è·ç¦»ï¼‰
                    distance = results['distances'][0][i]
                    similarity = max(0, 1 - distance / 2)  # ç®€å•çš„è·ç¦»åˆ°ç›¸ä¼¼åº¦è½¬æ¢
                    result_item["similarity"] = similarity
                    result_item["distance"] = distance
                
                formatted_results["results"].append(result_item)
        
        print(f"ğŸ” æŸ¥è¯¢: '{query}' - æ‰¾åˆ° {len(formatted_results['results'])} ä¸ªç»“æœ")
        return formatted_results
    
    def get_collection_info(self) -> Dict[str, Any]:
        """è·å–é›†åˆä¿¡æ¯"""
        count = self.collection.count()
        
        return {
            "name": self.collection_name,
            "count": count,
            "embedding_model": self.embedding_service.model,
            "embedding_dimension": self.embedding_service.get_embedding_dimension()
        }
    
    def get_knowledge_base_stats(self) -> Dict[str, Any]:
        """è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
        count = self.collection.count()
        
        return {
            "total_chunks": count,
            "collection_name": self.collection_name,
            "embedding_model": self.embedding_service.model,
            "embedding_dimension": self.embedding_service.get_embedding_dimension()
        }
    
    def search_documents(self, query: str, top_k: int = 5, similarity_threshold: float = 0.3):
        """
        æœç´¢æ–‡æ¡£ï¼ˆå…¼å®¹æ¥å£ï¼‰
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œä½äºæ­¤å€¼çš„ç»“æœå°†è¢«è¿‡æ»¤
            
        Returns:
            æ–‡æ¡£æºåˆ—è¡¨ï¼Œå¦‚æœæ²¡æœ‰è¶…è¿‡é˜ˆå€¼çš„ç»“æœåˆ™è¿”å›ç©ºåˆ—è¡¨
        """
        from core.models import DocumentSource
        
        # ä½¿ç”¨ç°æœ‰çš„searchæ–¹æ³•ï¼Œè·å–æ›´å¤šç»“æœä»¥ä¾¿è¿‡æ»¤
        results = self.search(query, n_results=min(top_k * 2, 20), include_distances=True)
        
        # è½¬æ¢ä¸ºDocumentSourceæ ¼å¼å¹¶åº”ç”¨ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤
        sources = []
        for result in results["results"]:
            similarity_score = result.get("similarity", 0.0)
            
            # åªä¿ç•™ç›¸ä¼¼åº¦è¶…è¿‡é˜ˆå€¼çš„ç»“æœ
            if similarity_score >= similarity_threshold:
                source = DocumentSource(
                    title=result["metadata"].get("source_file", "æœªçŸ¥æ–‡æ¡£"),
                    content=result["content"],
                    source=result["metadata"].get("source_file", "æœªçŸ¥æ¥æº"),
                    similarity=similarity_score,
                    metadata=result["metadata"],
                    file_name=result["metadata"].get("source_file", "æœªçŸ¥æ–‡æ¡£"),
                    regulation_code=result["metadata"].get("regulation_code"),
                    section=result["metadata"].get("section"),
                    similarity_score=similarity_score
                )
                sources.append(source)
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶è¿”å›å‰top_kä¸ªç»“æœ
        sources.sort(key=lambda x: x.similarity_score, reverse=True)
        filtered_sources = sources[:top_k]
        
        # è®°å½•è¿‡æ»¤ä¿¡æ¯
        total_found = len(results["results"])
        filtered_count = len(filtered_sources)
        print(f"ğŸ” æŸ¥è¯¢: '{query}' - æ€»å…±æ‰¾åˆ° {total_found} ä¸ªç»“æœï¼Œè¿‡æ»¤åä¿ç•™ {filtered_count} ä¸ªç»“æœï¼ˆé˜ˆå€¼: {similarity_threshold:.2f}ï¼‰")
        
        return filtered_sources
    
    def clear_collection(self):
        """æ¸…ç©ºé›†åˆ"""
        try:
            self.client.delete_collection(name=self.collection_name)
            self.collection = self._get_or_create_collection()
            print(f"ğŸ—‘ï¸ å·²æ¸…ç©ºé›†åˆ: {self.collection_name}")
        except Exception as e:
            print(f"âŒ æ¸…ç©ºé›†åˆå¤±è´¥: {e}")
    
    def remove_documents_by_source(self, source_file: str) -> int:
        """
        æ ¹æ®æ¥æºæ–‡ä»¶åˆ é™¤æ–‡æ¡£
        
        Args:
            source_file: æ¥æºæ–‡ä»¶å
            
        Returns:
            åˆ é™¤çš„æ–‡æ¡£æ•°é‡
        """
        try:
            # å…ˆæŸ¥è¯¢è¦åˆ é™¤çš„æ–‡æ¡£
            results = self.collection.get(
                where={"source_file": source_file}
            )
            
            if not results['ids']:
                print(f"âš ï¸ æœªæ‰¾åˆ°æ¥æºä¸º '{source_file}' çš„æ–‡æ¡£")
                return 0
            
            # åˆ é™¤æ–‡æ¡£
            self.collection.delete(
                where={"source_file": source_file}
            )
            
            removed_count = len(results['ids'])
            print(f"ğŸ—‘ï¸ æˆåŠŸåˆ é™¤ {removed_count} ä¸ªæ–‡æ¡£å—ï¼ˆæ¥æº: {source_file}ï¼‰")
            return removed_count
            
        except Exception as e:
            print(f"âŒ åˆ é™¤æ–‡æ¡£å¤±è´¥: {e}")
            raise
    
    def remove_documents_by_ids(self, doc_ids: List[str]) -> int:
        """
        æ ¹æ®æ–‡æ¡£IDåˆ é™¤æ–‡æ¡£
        
        Args:
            doc_ids: æ–‡æ¡£IDåˆ—è¡¨
            
        Returns:
            åˆ é™¤çš„æ–‡æ¡£æ•°é‡
        """
        try:
            if not doc_ids:
                return 0
            
            self.collection.delete(ids=doc_ids)
            
            print(f"ğŸ—‘ï¸ æˆåŠŸåˆ é™¤ {len(doc_ids)} ä¸ªæ–‡æ¡£å—")
            return len(doc_ids)
            
        except Exception as e:
            print(f"âŒ åˆ é™¤æ–‡æ¡£å¤±è´¥: {e}")
            raise
    
    def update_document(self, content: str, metadata: Dict[str, Any] = None, doc_id: str = None) -> str:
        """
        æ›´æ–°æ–‡æ¡£ï¼ˆå…ˆåˆ é™¤å†æ·»åŠ ï¼‰
        
        Args:
            content: æ–°çš„æ–‡æ¡£å†…å®¹
            metadata: æ–°çš„å…ƒæ•°æ®
            doc_id: è¦æ›´æ–°çš„æ–‡æ¡£IDï¼Œå¦‚æœä¸ºNoneåˆ™æ ¹æ®contentç”Ÿæˆ
            
        Returns:
            æ›´æ–°åçš„æ–‡æ¡£ID
        """
        if doc_id is None:
            doc_id = f"doc_{hash(content) % 1000000}"
        
        try:
            # å…ˆåˆ é™¤ç°æœ‰æ–‡æ¡£
            self.collection.delete(ids=[doc_id])
            print(f"ğŸ”„ åˆ é™¤æ—§æ–‡æ¡£: {doc_id}")
        except Exception:
            # å¦‚æœæ–‡æ¡£ä¸å­˜åœ¨ï¼Œç»§ç»­æ·»åŠ æ–°æ–‡æ¡£
            pass
        
        # æ·»åŠ æ–°æ–‡æ¡£
        return self.add_document(content, metadata)
    
    def get_documents_by_source(self, source_file: str) -> List[Dict[str, Any]]:
        """
        è·å–æŒ‡å®šæ¥æºçš„æ‰€æœ‰æ–‡æ¡£
        
        Args:
            source_file: æ¥æºæ–‡ä»¶å
            
        Returns:
            æ–‡æ¡£åˆ—è¡¨
        """
        try:
            results = self.collection.get(
                where={"source_file": source_file},
                include=['documents', 'metadatas', 'ids']
            )
            
            documents = []
            if results['ids']:
                for i, doc_id in enumerate(results['ids']):
                    documents.append({
                        "id": doc_id,
                        "content": results['documents'][i],
                        "metadata": results['metadatas'][i]
                    })
            
            print(f"ğŸ“‹ æ‰¾åˆ° {len(documents)} ä¸ªæ–‡æ¡£ï¼ˆæ¥æº: {source_file}ï¼‰")
            return documents
            
        except Exception as e:
            print(f"âŒ è·å–æ–‡æ¡£å¤±è´¥: {e}")
            return []
    
    def split_document(self, content: str, chunk_size: int = 1000, chunk_overlap: int = 200) -> List[str]:
        """
        åˆ†å‰²æ–‡æ¡£ä¸ºå°å—
        
        Args:
            content: æ–‡æ¡£å†…å®¹
            chunk_size: å—å¤§å°
            chunk_overlap: é‡å å¤§å°
            
        Returns:
            æ–‡æ¡£å—åˆ—è¡¨
        """
        if len(content) <= chunk_size:
            return [content]
        
        chunks = []
        start = 0
        
        while start < len(content):
            # ç¡®å®šç»“æŸä½ç½®
            end = start + chunk_size
            
            # å¦‚æœä¸æ˜¯æœ€åä¸€å—ï¼Œå°è¯•åœ¨å¥å·æˆ–æ¢è¡Œç¬¦å¤„åˆ†å‰²
            if end < len(content):
                # å¯»æ‰¾æœ€è¿‘çš„å¥å·æˆ–æ¢è¡Œç¬¦
                for i in range(end, start + chunk_size - 100, -1):
                    if content[i] in 'ã€‚\n':
                        end = i + 1
                        break
            
            chunk = content[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            # è®¾ç½®ä¸‹ä¸€ä¸ªå¼€å§‹ä½ç½®ï¼ˆè€ƒè™‘é‡å ï¼‰
            start = end - chunk_overlap
            if start >= len(content):
                break
        
        return chunks

def build_knowledge_base_from_file(file_path: str, api_key: str) -> BigModelKnowledgeBase:
    """
    ä»æ–‡ä»¶æ„å»ºçŸ¥è¯†åº“
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        api_key: BigModel APIå¯†é’¥
        
    Returns:
        çŸ¥è¯†åº“ç®¡ç†å™¨
    """
    # åˆå§‹åŒ–çŸ¥è¯†åº“
    kb = BigModelKnowledgeBase(api_key)
    
    # è¯»å–æ–‡ä»¶
    print(f"ğŸ“– è¯»å–æ–‡ä»¶: {file_path}")
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # åˆ†å‰²æ–‡æ¡£
    print(f"âœ‚ï¸ åˆ†å‰²æ–‡æ¡£...")
    chunks = kb.split_document(content)
    print(f"   åˆ†å‰²ä¸º {len(chunks)} ä¸ªå—")
    
    # å‡†å¤‡å…ƒæ•°æ®
    metadatas = []
    for i, chunk in enumerate(chunks):
        metadata = {
            "source_file": os.path.basename(file_path),
            "chunk_index": i,
            "chunk_count": len(chunks)
        }
        metadatas.append(metadata)
    
    # æ‰¹é‡æ·»åŠ åˆ°çŸ¥è¯†åº“
    kb.add_documents_batch(chunks, metadatas)
    
    return kb

if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    try:
        # è·å–APIå¯†é’¥
        api_key = input("è¯·è¾“å…¥BigModel APIå¯†é’¥: ").strip()
        if not api_key:
            print("âŒ æœªæä¾›APIå¯†é’¥")
            exit(1)
        
        # æµ‹è¯•çŸ¥è¯†åº“
        print("\nğŸ§ª æµ‹è¯•BigModelçŸ¥è¯†åº“...")
        kb = BigModelKnowledgeBase(api_key, "test_collection")
        
        # æ·»åŠ æµ‹è¯•æ–‡æ¡£
        test_docs = [
            "å¤–åŠ å‰‚æ˜¯æŒ‡åœ¨æ··å‡åœŸæ…æ‹Œè¿‡ç¨‹ä¸­æºå…¥çš„ç”¨äºæ”¹å–„æ··å‡åœŸæ€§èƒ½çš„åŒ–å­¦ç‰©è´¨ã€‚",
            "å‡æ°´å‰‚çš„ä¸»è¦ä½œç”¨æ˜¯å‡å°‘æ··å‡åœŸæ‹Œåˆç”¨æ°´é‡ï¼Œæé«˜å·¥ä½œæ€§èƒ½ã€‚",
            "HPWRæ˜¯é«˜æ€§èƒ½å‡æ°´å‰‚çš„è‹±æ–‡ç¼©å†™ï¼Œå…¨ç§°ä¸ºHigh Performance Water Reducerã€‚"
        ]
        
        kb.add_documents_batch(test_docs)
        
        # æµ‹è¯•æœç´¢
        test_queries = ["ä»€ä¹ˆæ˜¯å¤–åŠ å‰‚", "å‡æ°´å‰‚çš„ä½œç”¨", "HPWRä»£è¡¨ä»€ä¹ˆ"]
        
        for query in test_queries:
            print(f"\nğŸ” æœç´¢: {query}")
            results = kb.search(query, n_results=2)
            
            for i, result in enumerate(results["results"]):
                print(f"   ç»“æœ{i+1}: ç›¸ä¼¼åº¦={result.get('similarity', 0):.3f}")
                print(f"   å†…å®¹: {result['content'][:100]}...")
        
        # æ˜¾ç¤ºé›†åˆä¿¡æ¯
        info = kb.get_collection_info()
        print(f"\nğŸ“Š é›†åˆä¿¡æ¯:")
        print(f"   æ–‡æ¡£æ•°é‡: {info['count']}")
        print(f"   å‘é‡ç»´åº¦: {info['embedding_dimension']}")
        
        print("\nâœ… æµ‹è¯•å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}") 