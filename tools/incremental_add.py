#!/usr/bin/env python3
"""
å¢é‡æ·»åŠ å‘é‡æ•°æ®å·¥å…·
æ”¯æŒæ·»åŠ æ–°æ–‡æ¡£ã€æ›´æ–°ç°æœ‰æ–‡æ¡£ã€åˆ é™¤æ–‡æ¡£ç­‰æ“ä½œ
"""

import os
import sys
import argparse
import json
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from services.bigmodel_knowledge_base import BigModelKnowledgeBase
from core.config import Config

class IncrementalDataManager:
    """å¢é‡æ•°æ®ç®¡ç†å™¨"""
    
    def __init__(self, api_key: str = None, collection_name: str = None):
        """åˆå§‹åŒ–ç®¡ç†å™¨"""
        self.config = Config()
        self.api_key = api_key or self.config.bigmodel_api_key
        self.collection_name = collection_name or "engineering_knowledge_bigmodel"
        
        if not self.api_key:
            raise ValueError("è¯·è®¾ç½®BigModel APIå¯†é’¥")
        
        # åˆå§‹åŒ–çŸ¥è¯†åº“
        self.kb = BigModelKnowledgeBase(self.api_key, self.collection_name)
        
        print(f"âœ… å¢é‡æ•°æ®ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
        print(f"   é›†åˆåç§°: {self.collection_name}")
        print(f"   APIå¯†é’¥: {self.api_key[:10]}...")
    
    def add_file(self, file_path: str, chunk_size: int = 800, chunk_overlap: int = 100) -> Dict[str, Any]:
        """
        æ·»åŠ å•ä¸ªæ–‡ä»¶åˆ°çŸ¥è¯†åº“
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            chunk_size: å—å¤§å°
            chunk_overlap: é‡å å¤§å°
            
        Returns:
            æ·»åŠ ç»“æœ
        """
        file_path = Path(file_path)
        
        if not file_path.exists():
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        # æ£€æŸ¥æ–‡ä»¶ç±»å‹
        supported_types = ['.txt', '.md']
        if file_path.suffix.lower() not in supported_types:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_path.suffix}")
        
        print(f"ğŸ“– è¯»å–æ–‡ä»¶: {file_path}")
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except UnicodeDecodeError:
            # å°è¯•å…¶ä»–ç¼–ç 
            with open(file_path, 'r', encoding='gbk') as f:
                content = f.read()
        
        print(f"   æ–‡ä»¶å¤§å°: {len(content):,} å­—ç¬¦")
        
        # åˆ†å‰²æ–‡æ¡£
        chunks = self.kb.split_document(content, chunk_size, chunk_overlap)
        print(f"   åˆ†å‰²ä¸º {len(chunks)} ä¸ªå—")
        
        # å‡†å¤‡å…ƒæ•°æ®
        metadatas = []
        for i, chunk in enumerate(chunks):
            metadata = {
                "source_file": file_path.name,
                "file_path": str(file_path),
                "chunk_index": i,
                "chunk_count": len(chunks),
                "add_time": datetime.now().isoformat(),
                "content_length": len(chunk),
                "file_size": len(content)
            }
            metadatas.append(metadata)
        
        # æ‰¹é‡æ·»åŠ 
        print(f"ğŸ”„ æ·»åŠ åˆ°çŸ¥è¯†åº“...")
        doc_ids = self.kb.add_documents_batch(chunks, metadatas)
        
        result = {
            "file_path": str(file_path),
            "chunks_added": len(doc_ids),
            "document_ids": doc_ids,
            "success": True
        }
        
        print(f"âœ… æˆåŠŸæ·»åŠ æ–‡ä»¶: {file_path.name}")
        print(f"   æ·»åŠ äº† {len(doc_ids)} ä¸ªæ–‡æ¡£å—")
        
        return result
    
    def add_directory(self, dir_path: str, recursive: bool = True, 
                     chunk_size: int = 800, chunk_overlap: int = 100) -> Dict[str, Any]:
        """
        æ·»åŠ ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶
        
        Args:
            dir_path: ç›®å½•è·¯å¾„
            recursive: æ˜¯å¦é€’å½’å¤„ç†å­ç›®å½•
            chunk_size: å—å¤§å°
            chunk_overlap: é‡å å¤§å°
            
        Returns:
            æ·»åŠ ç»“æœ
        """
        dir_path = Path(dir_path)
        
        if not dir_path.exists() or not dir_path.is_dir():
            raise ValueError(f"ç›®å½•ä¸å­˜åœ¨æˆ–ä¸æ˜¯ç›®å½•: {dir_path}")
        
        print(f"ğŸ“ å¤„ç†ç›®å½•: {dir_path}")
        
        # æŸ¥æ‰¾æ”¯æŒçš„æ–‡ä»¶
        supported_types = ['.txt', '.md']
        files = []
        
        if recursive:
            for ext in supported_types:
                files.extend(dir_path.rglob(f"*{ext}"))
        else:
            for ext in supported_types:
                files.extend(dir_path.glob(f"*{ext}"))
        
        print(f"   æ‰¾åˆ° {len(files)} ä¸ªæ”¯æŒçš„æ–‡ä»¶")
        
        if not files:
            return {"success": False, "message": "æœªæ‰¾åˆ°æ”¯æŒçš„æ–‡ä»¶"}
        
        results = []
        total_chunks = 0
        successful_files = 0
        
        for file_path in files:
            try:
                print(f"\nå¤„ç†æ–‡ä»¶: {file_path.name}")
                result = self.add_file(file_path, chunk_size, chunk_overlap)
                results.append(result)
                total_chunks += result["chunks_added"]
                successful_files += 1
                
            except Exception as e:
                print(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥: {file_path.name} - {e}")
                results.append({
                    "file_path": str(file_path),
                    "success": False,
                    "error": str(e)
                })
        
        summary = {
            "directory": str(dir_path),
            "total_files": len(files),
            "successful_files": successful_files,
            "total_chunks": total_chunks,
            "results": results,
            "success": successful_files > 0
        }
        
        print(f"\nğŸ“Š ç›®å½•å¤„ç†å®Œæˆ:")
        print(f"   æˆåŠŸå¤„ç†: {successful_files}/{len(files)} ä¸ªæ–‡ä»¶")
        print(f"   æ€»æ–‡æ¡£å—: {total_chunks}")
        
        return summary
    
    def add_text(self, text: str, title: str = "æ‰‹åŠ¨æ·»åŠ ", 
                chunk_size: int = 800, chunk_overlap: int = 100) -> Dict[str, Any]:
        """
        ç›´æ¥æ·»åŠ æ–‡æœ¬å†…å®¹
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            title: æ–‡æ¡£æ ‡é¢˜
            chunk_size: å—å¤§å°
            chunk_overlap: é‡å å¤§å°
            
        Returns:
            æ·»åŠ ç»“æœ
        """
        if not text.strip():
            raise ValueError("æ–‡æœ¬å†…å®¹ä¸èƒ½ä¸ºç©º")
        
        print(f"ğŸ“ æ·»åŠ æ–‡æœ¬: {title}")
        print(f"   æ–‡æœ¬é•¿åº¦: {len(text):,} å­—ç¬¦")
        
        # åˆ†å‰²æ–‡æ¡£
        chunks = self.kb.split_document(text, chunk_size, chunk_overlap)
        print(f"   åˆ†å‰²ä¸º {len(chunks)} ä¸ªå—")
        
        # å‡†å¤‡å…ƒæ•°æ®
        metadatas = []
        for i, chunk in enumerate(chunks):
            metadata = {
                "source_file": title,
                "chunk_index": i,
                "chunk_count": len(chunks),
                "document_type": "manual",
                "add_time": datetime.now().isoformat(),
                "content_length": len(chunk)
            }
            metadatas.append(metadata)
        
        # æ‰¹é‡æ·»åŠ 
        print(f"ğŸ”„ æ·»åŠ åˆ°çŸ¥è¯†åº“...")
        doc_ids = self.kb.add_documents_batch(chunks, metadatas)
        
        result = {
            "title": title,
            "chunks_added": len(doc_ids),
            "document_ids": doc_ids,
            "success": True
        }
        
        print(f"âœ… æˆåŠŸæ·»åŠ æ–‡æœ¬")
        print(f"   æ·»åŠ äº† {len(doc_ids)} ä¸ªæ–‡æ¡£å—")
        
        return result
    
    def update_file(self, file_path: str, chunk_size: int = 800, chunk_overlap: int = 100) -> Dict[str, Any]:
        """
        æ›´æ–°æ–‡ä»¶ï¼ˆå…ˆåˆ é™¤æ—§ç‰ˆæœ¬ï¼Œå†æ·»åŠ æ–°ç‰ˆæœ¬ï¼‰
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            chunk_size: å—å¤§å°
            chunk_overlap: é‡å å¤§å°
            
        Returns:
            æ›´æ–°ç»“æœ
        """
        file_path = Path(file_path)
        filename = file_path.name
        
        print(f"ğŸ”„ æ›´æ–°æ–‡ä»¶: {filename}")
        
        # å…ˆåˆ é™¤ç°æœ‰æ–‡æ¡£
        try:
            removed_count = self.kb.remove_documents_by_source(filename)
            print(f"   åˆ é™¤äº† {removed_count} ä¸ªæ—§æ–‡æ¡£å—")
        except Exception as e:
            print(f"   åˆ é™¤æ—§æ–‡æ¡£æ—¶å‡ºç°è­¦å‘Š: {e}")
        
        # æ·»åŠ æ–°æ–‡æ¡£
        result = self.add_file(file_path, chunk_size, chunk_overlap)
        result["removed_count"] = removed_count if 'removed_count' in locals() else 0
        result["operation"] = "update"
        
        print(f"âœ… æ–‡ä»¶æ›´æ–°å®Œæˆ: {filename}")
        
        return result
    
    def remove_file(self, filename: str) -> Dict[str, Any]:
        """
        åˆ é™¤æŒ‡å®šæ–‡ä»¶çš„æ‰€æœ‰æ–‡æ¡£
        
        Args:
            filename: æ–‡ä»¶å
            
        Returns:
            åˆ é™¤ç»“æœ
        """
        print(f"ğŸ—‘ï¸ åˆ é™¤æ–‡ä»¶: {filename}")
        
        try:
            removed_count = self.kb.remove_documents_by_source(filename)
            
            result = {
                "filename": filename,
                "removed_count": removed_count,
                "success": removed_count > 0
            }
            
            if removed_count > 0:
                print(f"âœ… æˆåŠŸåˆ é™¤ {removed_count} ä¸ªæ–‡æ¡£å—")
            else:
                print(f"âš ï¸ æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
            
            return result
            
        except Exception as e:
            print(f"âŒ åˆ é™¤å¤±è´¥: {e}")
            return {
                "filename": filename,
                "success": False,
                "error": str(e)
            }
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
        return self.kb.get_knowledge_base_stats()
    
    def search_test(self, query: str, top_k: int = 5) -> Dict[str, Any]:
        """æµ‹è¯•æœç´¢åŠŸèƒ½"""
        print(f"ğŸ” æœç´¢æµ‹è¯•: {query}")
        
        results = self.kb.search(query, n_results=top_k, include_distances=True)
        
        print(f"   æ‰¾åˆ° {len(results['results'])} ä¸ªç»“æœ")
        
        for i, result in enumerate(results["results"]):
            similarity = result.get('similarity', 0)
            print(f"   ç»“æœ{i+1}: ç›¸ä¼¼åº¦={similarity:.3f}")
            print(f"   æ¥æº: {result['metadata'].get('source_file', 'æœªçŸ¥')}")
            print(f"   å†…å®¹: {result['content'][:100]}...")
        
        return results

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="å¢é‡æ·»åŠ å‘é‡æ•°æ®å·¥å…·")
    parser.add_argument("action", choices=[
        "add-file", "add-dir", "add-text", "update-file", 
        "remove-file", "stats", "search"
    ], help="æ“ä½œç±»å‹")
    
    parser.add_argument("--path", help="æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„")
    parser.add_argument("--text", help="è¦æ·»åŠ çš„æ–‡æœ¬å†…å®¹")
    parser.add_argument("--title", default="æ‰‹åŠ¨æ·»åŠ ", help="æ–‡æœ¬æ ‡é¢˜")
    parser.add_argument("--filename", help="æ–‡ä»¶åï¼ˆç”¨äºåˆ é™¤ï¼‰")
    parser.add_argument("--query", help="æœç´¢æŸ¥è¯¢")
    
    parser.add_argument("--collection", default="engineering_knowledge_bigmodel", help="é›†åˆåç§°")
    parser.add_argument("--chunk-size", type=int, default=800, help="å—å¤§å°")
    parser.add_argument("--chunk-overlap", type=int, default=100, help="é‡å å¤§å°")
    parser.add_argument("--recursive", action="store_true", help="é€’å½’å¤„ç†å­ç›®å½•")
    parser.add_argument("--top-k", type=int, default=5, help="æœç´¢è¿”å›ç»“æœæ•°é‡")
    
    args = parser.parse_args()
    
    try:
        # åˆå§‹åŒ–ç®¡ç†å™¨
        manager = IncrementalDataManager(collection_name=args.collection)
        
        # æ‰§è¡Œæ“ä½œ
        if args.action == "add-file":
            if not args.path:
                print("âŒ è¯·ä½¿ç”¨ --path æŒ‡å®šæ–‡ä»¶è·¯å¾„")
                sys.exit(1)
            result = manager.add_file(args.path, args.chunk_size, args.chunk_overlap)
            
        elif args.action == "add-dir":
            if not args.path:
                print("âŒ è¯·ä½¿ç”¨ --path æŒ‡å®šç›®å½•è·¯å¾„")
                sys.exit(1)
            result = manager.add_directory(args.path, args.recursive, args.chunk_size, args.chunk_overlap)
            
        elif args.action == "add-text":
            if not args.text:
                print("âŒ è¯·ä½¿ç”¨ --text æŒ‡å®šè¦æ·»åŠ çš„æ–‡æœ¬å†…å®¹")
                sys.exit(1)
            result = manager.add_text(args.text, args.title, args.chunk_size, args.chunk_overlap)
            
        elif args.action == "update-file":
            if not args.path:
                print("âŒ è¯·ä½¿ç”¨ --path æŒ‡å®šæ–‡ä»¶è·¯å¾„")
                sys.exit(1)
            result = manager.update_file(args.path, args.chunk_size, args.chunk_overlap)
            
        elif args.action == "remove-file":
            if not args.filename:
                print("âŒ è¯·ä½¿ç”¨ --filename æŒ‡å®šè¦åˆ é™¤çš„æ–‡ä»¶å")
                sys.exit(1)
            result = manager.remove_file(args.filename)
            
        elif args.action == "stats":
            result = manager.get_stats()
            print(f"\nğŸ“Š çŸ¥è¯†åº“ç»Ÿè®¡:")
            print(f"   é›†åˆåç§°: {result['collection_name']}")
            print(f"   æ–‡æ¡£æ€»æ•°: {result['total_chunks']}")
            print(f"   å‘é‡æ¨¡å‹: {result['embedding_model']}")
            print(f"   å‘é‡ç»´åº¦: {result['embedding_dimension']}")
            
        elif args.action == "search":
            if not args.query:
                print("âŒ è¯·ä½¿ç”¨ --query æŒ‡å®šæœç´¢æŸ¥è¯¢")
                sys.exit(1)
            result = manager.search_test(args.query, args.top_k)
        
        # ä¿å­˜ç»“æœï¼ˆé™¤äº†statså’Œsearchï¼‰
        if args.action not in ["stats", "search"]:
            result_file = f"incremental_result_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {result_file}")
        
        print(f"\nğŸ‰ æ“ä½œå®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ æ“ä½œå¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main() 