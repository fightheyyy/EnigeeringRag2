"""
æ„å»ºå›½å®¶æ ‡å‡†çŸ¥è¯†åº“
ä¸“é—¨å¤„ç†å›½å®¶æ ‡å‡†åº“ç›®å½•ä¸‹çš„æ ‡å‡†æ–‡æ¡£ï¼Œå­˜å‚¨åˆ°"standards"é›†åˆ
"""

import os
import glob
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from services.bigmodel_knowledge_base import BigModelKnowledgeBase
from core.config import Config

def main():
    """ä¸»å‡½æ•°ï¼šæ„å»ºå›½å®¶æ ‡å‡†çŸ¥è¯†åº“"""
    print("ğŸ—ï¸ å¼€å§‹æ„å»ºå›½å®¶æ ‡å‡†çŸ¥è¯†åº“...")
    print("=" * 60)
    
    # é…ç½®
    config = Config()
    standards_dir = "./data/å›½å®¶æ ‡å‡†åº“"
    collection_name = "standards"  # ä¸“é—¨ç”¨äºå›½å®¶æ ‡å‡†çš„é›†åˆ
    
    print(f"ğŸ“ æ ‡å‡†æ–‡æ¡£ç›®å½•: {standards_dir}")
    print(f"ğŸ“š ç›®æ ‡é›†åˆåç§°: {collection_name}")
    
    # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(standards_dir):
        print(f"âŒ é”™è¯¯: ç›®å½• {standards_dir} ä¸å­˜åœ¨")
        return False
    
    try:
        # åˆå§‹åŒ–çŸ¥è¯†åº“ç®¡ç†å™¨ï¼ˆä½¿ç”¨standardsé›†åˆï¼‰
        print(f"\nğŸ”§ åˆå§‹åŒ–çŸ¥è¯†åº“ç®¡ç†å™¨...")
        kb = BigModelKnowledgeBase(
            api_key=config.bigmodel_api_key,
            collection_name=collection_name
        )
        
        # æ–‡æ¡£å¤„ç†é…ç½®
        print(f"ğŸ“„ å‡†å¤‡æ–‡æ¡£å¤„ç†...")
        
        # è·å–æ‰€æœ‰txtæ–‡ä»¶
        txt_files = glob.glob(os.path.join(standards_dir, "*.txt"))
        print(f"\nğŸ“‹ æ‰¾åˆ° {len(txt_files)} ä¸ªæ ‡å‡†æ–‡æ¡£")
        
        if not txt_files:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•txtæ–‡ä»¶")
            return False
        
        # æ˜¾ç¤ºæ–‡ä»¶åˆ—è¡¨
        print("\nğŸ“„ æ ‡å‡†æ–‡æ¡£åˆ—è¡¨:")
        for i, file_path in enumerate(txt_files, 1):
            file_name = os.path.basename(file_path)
            file_size = os.path.getsize(file_path) / 1024  # KB
            print(f"   {i:2d}. {file_name} ({file_size:.1f}KB)")
        
        # æ¸…ç©ºç°æœ‰é›†åˆï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        print(f"\nğŸ—‘ï¸ æ¸…ç©ºç°æœ‰çš„ '{collection_name}' é›†åˆ...")
        try:
            kb.clear_collection()
        except Exception as e:
            print(f"   æ³¨æ„: {e}")
        
        # å¤„ç†æ¯ä¸ªæ–‡æ¡£
        total_chunks = 0
        successful_files = 0
        
        for i, file_path in enumerate(txt_files, 1):
            file_name = os.path.basename(file_path)
            print(f"\nğŸ“– å¤„ç†æ–‡ä»¶ {i}/{len(txt_files)}: {file_name}")
            
            try:
                # è¯»å–æ–‡ä»¶å†…å®¹
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read().strip()
                
                if not content:
                    print(f"   âš ï¸ è·³è¿‡ç©ºæ–‡ä»¶: {file_name}")
                    continue
                
                print(f"   ğŸ“ æ–‡æ¡£é•¿åº¦: {len(content)} å­—ç¬¦")
                
                # åˆ†å‰²æ–‡æ¡£
                chunks = kb.split_document(
                    content, 
                    chunk_size=config.DOCUMENT_CONFIG["chunk_size"],
                    chunk_overlap=config.DOCUMENT_CONFIG["chunk_overlap"]
                )
                
                print(f"   âœ‚ï¸ åˆ†å‰²ä¸º {len(chunks)} ä¸ªå—")
                
                # å‡†å¤‡å…ƒæ•°æ®
                metadatas = []
                for j, chunk in enumerate(chunks):
                    # ä»æ–‡ä»¶åæå–æ ‡å‡†ä¿¡æ¯
                    standard_number = file_name.replace('.txt', '').replace('+', ' ')
                    
                    metadata = {
                        "source_file": file_name,
                        "standard_number": standard_number,
                        "document_type": "national_standard",
                        "chunk_index": j,
                        "chunk_count": len(chunks),
                        "file_size": len(content),
                        "content_preview": chunk[:100] + "..." if len(chunk) > 100 else chunk
                    }
                    metadatas.append(metadata)
                
                # æ‰¹é‡æ·»åŠ åˆ°çŸ¥è¯†åº“
                print(f"   ğŸ”„ æ·»åŠ åˆ°çŸ¥è¯†åº“...")
                doc_ids = kb.add_documents_batch(chunks, metadatas)
                
                print(f"   âœ… æˆåŠŸæ·»åŠ  {len(doc_ids)} ä¸ªæ–‡æ¡£å—")
                total_chunks += len(chunks)
                successful_files += 1
                
            except Exception as e:
                print(f"   âŒ å¤„ç†æ–‡ä»¶å¤±è´¥: {e}")
                continue
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        print(f"\n" + "=" * 60)
        print(f"ğŸ‰ å›½å®¶æ ‡å‡†çŸ¥è¯†åº“æ„å»ºå®Œæˆ!")
        print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡:")
        print(f"   - æˆåŠŸå¤„ç†æ–‡ä»¶: {successful_files}/{len(txt_files)}")
        print(f"   - æ€»æ–‡æ¡£å—æ•°: {total_chunks}")
        
        # è·å–é›†åˆä¿¡æ¯
        info = kb.get_collection_info()
        print(f"\nğŸ“š çŸ¥è¯†åº“ä¿¡æ¯:")
        print(f"   - é›†åˆåç§°: {info['name']}")
        print(f"   - æ–‡æ¡£æ€»æ•°: {info['count']}")
        print(f"   - å‘é‡æ¨¡å‹: {info['embedding_model']}")
        print(f"   - å‘é‡ç»´åº¦: {info['embedding_dimension']}")
        
        # æµ‹è¯•æœç´¢åŠŸèƒ½
        print(f"\nğŸ§ª æµ‹è¯•æœç´¢åŠŸèƒ½...")
        test_queries = [
            "æ··å‡åœŸå¤–åŠ å‰‚",
            "æ°´æ³¥",
            "å»ºç­‘ææ–™",
            "è´¨é‡æ ‡å‡†",
            "æŠ€æœ¯è¦æ±‚"
        ]
        
        for query in test_queries:
            print(f"\nğŸ” æœç´¢: '{query}'")
            try:
                results = kb.search(query, n_results=3)
                
                if results["results"]:
                    for j, result in enumerate(results["results"]):
                        similarity = result.get('similarity', 0)
                        source = result['metadata'].get('standard_number', 'æœªçŸ¥æ ‡å‡†')
                        print(f"   ç»“æœ{j+1}: {source} (ç›¸ä¼¼åº¦={similarity:.3f})")
                        print(f"   å†…å®¹: {result['content'][:80]}...")
                else:
                    print("   æœªæ‰¾åˆ°ç›¸å…³ç»“æœ")
                    
            except Exception as e:
                print(f"   æœç´¢å¤±è´¥: {e}")
        
        print(f"\nâœ¨ å›½å®¶æ ‡å‡†çŸ¥è¯†åº“å·²å‡†å¤‡å°±ç»ªï¼")
        print(f"ğŸ’¡ æ‚¨ç°åœ¨å¯ä»¥é€šè¿‡é›†åˆåç§° '{collection_name}' è®¿é—®è¿™ä¸ªçŸ¥è¯†åº“")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ„å»ºçŸ¥è¯†åº“å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def get_standards_collection_info():
    """è·å–æ ‡å‡†çŸ¥è¯†åº“ä¿¡æ¯"""
    try:
        config = Config()
        kb = BigModelKnowledgeBase(
            api_key=config.bigmodel_api_key,
            collection_name="standards"
        )
        return kb.get_collection_info()
    except Exception as e:
        print(f"è·å–ä¿¡æ¯å¤±è´¥: {e}")
        return None

if __name__ == "__main__":
    print("ğŸ—ï¸ å›½å®¶æ ‡å‡†çŸ¥è¯†åº“æ„å»ºå·¥å…·")
    print("=" * 60)
    
    success = main()
    
    if success:
        print(f"\nğŸ¯ ä¸‹ä¸€æ­¥å»ºè®®:")
        print(f"   1. å¯ä»¥åˆ›å»º'regulations'é›†åˆç”¨äºæ³•å¾‹æ³•è§„")
        print(f"   2. å¯ä»¥åˆ›å»º'drawings'é›†åˆç”¨äºé¡¹ç›®å›¾çº¸")
        print(f"   3. åœ¨ä¸»ç³»ç»Ÿä¸­é€‰æ‹©ä½¿ç”¨'standards'é›†åˆ")
    else:
        print(f"\nâŒ æ„å»ºå¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯å¹¶é‡è¯•") 