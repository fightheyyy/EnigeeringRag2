from fastapi import FastAPI, HTTPException, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse, FileResponse
from datetime import datetime
import uuid
import logging
import os
import re
from typing import List, Optional

from core.config import Config
from core.models import QuestionRequest, AnswerResponse, KnowledgeDocument, SystemStatus  
from services.bigmodel_knowledge_base import BigModelKnowledgeBase as KnowledgeBaseManager
from services.llm_service import LLMService, enhance_engineering_question
from services.mysql_standards_service import get_mysql_standards_service

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# åˆå§‹åŒ–FastAPIåº”ç”¨
app = FastAPI(
    title="å·¥ç¨‹ç›‘ç†æ™ºèƒ½é—®ç­”ç³»ç»Ÿ",
    description="ä¸ºç°åœºç›‘ç†å·¥ç¨‹å¸ˆæä¾›è§„èŒƒå’Œå›¾çº¸æŸ¥è¯¢æœåŠ¡",
    version="1.0.0"
)

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æŒ‚è½½é™æ€æ–‡ä»¶ç›®å½•
app.mount("/static", StaticFiles(directory="static"), name="static")

# åˆå§‹åŒ–æœåŠ¡
config = Config()

# çŸ¥è¯†åº“é…ç½® - æ”¯æŒå¤šä¸ªä¸“é—¨çš„çŸ¥è¯†åº“
KNOWLEDGE_BASES = {
    "standards": "å›½å®¶æ ‡å‡†åº“",
    "engineering_knowledge_base": "åŸæœ‰å·¥ç¨‹çŸ¥è¯†åº“", 
    "regulations": "æ³•å¾‹æ³•è§„åº“",
    "drawings": "é¡¹ç›®å›¾çº¸åº“"      # é¢„ç•™
}

# é»˜è®¤ä½¿ç”¨standardsé›†åˆï¼ˆå›½å®¶æ ‡å‡†åº“ï¼‰
DEFAULT_COLLECTION = "standards"

# ä½¿ç”¨BigModelçŸ¥è¯†åº“ï¼ŒæŒ‡å®šstandardsé›†åˆ
kb_manager = KnowledgeBaseManager(
    api_key=config.bigmodel_api_key,
    collection_name=DEFAULT_COLLECTION
)
llm_service = LLMService()

# åˆå§‹åŒ–MySQLæ ‡å‡†æœåŠ¡
try:
    standards_service = get_mysql_standards_service()
    logger.info("âœ… MySQLæ ‡å‡†æ•°æ®åº“æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
except Exception as e:
    logger.error(f"âŒ MySQLæ ‡å‡†æ•°æ®åº“æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
    standards_service = None

# å­˜å‚¨ä¼šè¯å†å²ï¼ˆç”Ÿäº§ç¯å¢ƒä¸­åº”ä½¿ç”¨æ•°æ®åº“ï¼‰
session_history = {}

def extract_used_standards_from_answer(answer: str) -> List[str]:
    """ä»ç­”æ¡ˆä¸­æå–DeepSeekæ ‡æ³¨çš„ä½¿ç”¨æ ‡å‡†"""
    # æŸ¥æ‰¾[ä½¿ç”¨æ ‡å‡†: XXX]æ ¼å¼çš„æ ‡æ³¨
    pattern = r'\[ä½¿ç”¨æ ‡å‡†:\s*([^\]]+)\]'
    match = re.search(pattern, answer)
    
    if match:
        standards_text = match.group(1).strip()
        if standards_text.lower() == "æ— ":
            return []
        
        # åˆ†å‰²å¤šä¸ªæ ‡å‡†ï¼ˆç”¨é€—å·åˆ†éš”ï¼‰
        standards = [std.strip() for std in standards_text.split(',')]
        return [std for std in standards if std]  # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²
    
    return []

def smart_filter_standards(answer: str, standards: List) -> List:
    """æ™ºèƒ½è¿‡æ»¤æ ‡å‡†ï¼šåŸºäºç­”æ¡ˆå†…å®¹è¿‡æ»¤å‡ºçœŸæ­£ç›¸å…³çš„æ ‡å‡†"""
    if not standards:
        return []
    
    # ä¸ç›¸å…³çš„æ ‡å‡†å…³é”®è¯
    irrelevant_keywords = [
        "æ°´æ•ˆé™å®šå€¼", "æ°´æ•ˆç­‰çº§", "åä¾¿å™¨", "è¹²ä¾¿å™¨", "å°ä¾¿å™¨", 
        "ä¾¿å™¨å†²æ´—é˜€", "èŠ‚æ°´", "ç”¨æ°´é‡", "å†²æ´—åŠŸèƒ½"
    ]
    
    # ç›¸å…³çš„æ ‡å‡†å…³é”®è¯
    relevant_keywords = [
        "åº”æ€¥é¿éš¾åœºæ‰€", "åœ°éœ‡åº”æ€¥", "åº”æ€¥å•æ‰€", "é¿éš¾åœºæ‰€", 
        "åœºå€åŠé…å¥—è®¾æ–½", "21734"
    ]
    
    filtered = []
    for standard in standards:
        standard_name = standard.standard_name.lower()
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸ç›¸å…³å…³é”®è¯
        is_irrelevant = any(keyword in standard_name for keyword in irrelevant_keywords)
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç›¸å…³å…³é”®è¯
        is_relevant = any(keyword in standard_name.lower() for keyword in relevant_keywords)
        
        # æ£€æŸ¥ç­”æ¡ˆä¸­æ˜¯å¦ç›´æ¥æåŠè¯¥æ ‡å‡†
        answer_lower = answer.lower()
        standard_mentioned = (
            standard.standard_number.lower() in answer_lower or
            standard.standard_number.replace("-", "").replace(" ", "").lower() in answer_lower.replace("-", "").replace(" ", "")
        )
        
        # å†³ç­–é€»è¾‘ï¼š
        # 1. å¦‚æœåœ¨ç­”æ¡ˆä¸­è¢«æ˜ç¡®æåŠï¼Œåˆ™åŒ…å«
        # 2. å¦‚æœåŒ…å«ç›¸å…³å…³é”®è¯ä¸”ä¸åŒ…å«ä¸ç›¸å…³å…³é”®è¯ï¼Œåˆ™åŒ…å«
        # 3. å¦‚æœåŒ…å«ä¸ç›¸å…³å…³é”®è¯ï¼Œåˆ™æ’é™¤
        if standard_mentioned or (is_relevant and not is_irrelevant):
            filtered.append(standard)
        elif is_irrelevant:
            # æ˜ç¡®æ’é™¤ä¸ç›¸å…³çš„æ ‡å‡†
            logger.info(f"è¿‡æ»¤æ‰ä¸ç›¸å…³æ ‡å‡†: {standard.standard_number} - {standard.standard_name}")
            continue
    
    # å¦‚æœè¿‡æ»¤åæ²¡æœ‰æ ‡å‡†ï¼Œä¿ç•™ç¬¬ä¸€ä¸ªï¼ˆé€šå¸¸æ˜¯æœ€ç›¸å…³çš„ï¼‰
    if not filtered and standards:
        filtered = [standards[0]]
    
    # æœ€å¤šè¿”å›2ä¸ªæ ‡å‡†ä»¥é¿å…ä¿¡æ¯è¿‡è½½
    return filtered[:2]

@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨æ—¶çš„åˆå§‹åŒ–"""
    logger.info("å·¥ç¨‹ç›‘ç†æ™ºèƒ½é—®ç­”ç³»ç»Ÿå¯åŠ¨ä¸­...")
    
    # æ˜¾ç¤ºå½“å‰çŸ¥è¯†åº“ä¿¡æ¯
    try:
        info = kb_manager.get_collection_info()
        logger.info(f"ğŸ“š å½“å‰çŸ¥è¯†åº“: {KNOWLEDGE_BASES[DEFAULT_COLLECTION]} ({DEFAULT_COLLECTION})")
        logger.info(f"ğŸ“Š æ–‡æ¡£æ•°é‡: {info.get('count', 0)} ä¸ª")
        logger.info(f"ğŸ¤– å‘é‡æ¨¡å‹: {info.get('embedding_model', 'unknown')}")
        logger.info(f"ğŸ“ å‘é‡ç»´åº¦: {info.get('embedding_dimension', 0)}")
    except Exception as e:
        logger.error(f"è·å–çŸ¥è¯†åº“ä¿¡æ¯å¤±è´¥: {e}")
    
    # æ˜¾ç¤ºMySQLæ ‡å‡†åº“çŠ¶æ€
    if standards_service:
        logger.info("âœ… MySQLæ ‡å‡†æ•°æ®åº“é›†æˆå·²å¯ç”¨")
    else:
        logger.warning("âš ï¸ MySQLæ ‡å‡†æ•°æ®åº“é›†æˆæœªå¯ç”¨")
    
    logger.info("ç³»ç»Ÿå¯åŠ¨å®Œæˆ")

@app.get("/", response_class=FileResponse)
async def get_homepage():
    """è¿”å›ä¸»é¡µ"""
    return FileResponse("static/index.html")

@app.get("/admin", response_class=FileResponse)
async def get_admin_page():
    """è¿”å›ç®¡ç†é¡µé¢"""
    return FileResponse("static/admin.html")

@app.post("/ask", response_model=AnswerResponse)
async def ask_question(request: QuestionRequest):
    """å¤„ç†ç”¨æˆ·é—®é¢˜"""
    try:
        logger.info(f"æ”¶åˆ°é—®é¢˜: {request.question}")
        
        # åˆå§‹åŒ–æ‰€æœ‰çŸ¥è¯†åº“ç®¡ç†å™¨
        standards_kb_manager = KnowledgeBaseManager(
            api_key=config.bigmodel_api_key,
            collection_name="standards"
        )
        regulations_kb_manager = KnowledgeBaseManager(
            api_key=config.bigmodel_api_key,
            collection_name="regulations"
        )
        
        # å¢å¼ºé—®é¢˜è¡¨è¿°
        enhanced_question = enhance_engineering_question(request.question)
        
        # å¤šé‡æ£€ç´¢ç­–ç•¥ï¼šä½¿ç”¨å¤šä¸ªæŸ¥è¯¢è¯æé«˜æ£€ç´¢å‡†ç¡®æ€§
        search_queries = [enhanced_question]
        
        # é’ˆå¯¹ç‰¹å®šé—®é¢˜æ·»åŠ æ›¿ä»£æŸ¥è¯¢è¯
        original_q = request.question.lower()
        if "åº”æ€¥å•æ‰€" in original_q and "è·ç¦»" in original_q:
            search_queries.extend([
                "åº”æ€¥å•æ‰€è®¾ç½®è¦æ±‚",
                "6.1.6 åº”æ€¥å•æ‰€",
                "åº”æ€¥é¿éš¾åœºæ‰€å•æ‰€è®¾ç½®",
                "GB 21734 åº”æ€¥å•æ‰€",
                "ç¯·å®¿åŒºå•æ‰€è·ç¦»"
            ])
        elif "å•æ‰€" in original_q and ("é—´è·" in original_q or "è·ç¦»" in original_q):
            search_queries.extend([
                "åº”æ€¥å•æ‰€è®¾ç½®è¦æ±‚",
                "å•æ‰€å¸ƒå±€è¦æ±‚"
            ])
        
        # æ‰§è¡Œå¤šé‡æ£€ç´¢å¹¶åˆå¹¶ç»“æœï¼ˆåŒæ—¶æœç´¢æ‰€æœ‰çŸ¥è¯†åº“ï¼‰
        all_results = []
        seen_content = set()  # é¿å…é‡å¤å†…å®¹
        
        logger.info("ğŸ” å¼€å§‹æ£€ç´¢æ‰€æœ‰çŸ¥è¯†åº“...")
        
        for query in search_queries:
            # æœç´¢å›½å®¶æ ‡å‡†åº“
            logger.info(f"ğŸ“Š æœç´¢standardsåº“: {query}")
            standards_result = standards_kb_manager.search(query, n_results=config.MAX_RETRIEVAL_RESULTS)
            
            if standards_result and "results" in standards_result:
                for result in standards_result["results"]:
                    content_hash = hash(result['content'][:100])
                    if content_hash not in seen_content:
                        seen_content.add(content_hash)
                        # æ ‡è®°æ¥æºä¸ºstandards
                        result['source_type'] = 'standards'
                        all_results.append(result)
            
            # æœç´¢æ³•è§„åº“
            logger.info(f"ğŸ›ï¸ æœç´¢regulationsåº“: {query}")
            regulations_result = regulations_kb_manager.search(query, n_results=config.MAX_RETRIEVAL_RESULTS)
            
            if regulations_result and "results" in regulations_result:
                for result in regulations_result["results"]:
                    content_hash = hash(result['content'][:100])
                    if content_hash not in seen_content:
                        seen_content.add(content_hash)
                        # æ ‡è®°æ¥æºä¸ºregulations
                        result['source_type'] = 'regulations'
                        all_results.append(result)
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶å–å‰Nä¸ªç»“æœ
        all_results.sort(key=lambda x: x.get('similarity', 0), reverse=True)
        final_results = all_results[:config.MAX_RETRIEVAL_RESULTS * 2]  # å…è®¸æ›´å¤šç»“æœç”¨äºè¿‡æ»¤
        
        # åŒ…è£…ä¸ºæ ‡å‡†æ ¼å¼
        sources_result = {"results": final_results}
        
        # å¤„ç†æœç´¢ç»“æœå¹¶åº”ç”¨ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤
        sources = []
        if sources_result and "results" in sources_result:
            for result in sources_result["results"]:
                similarity = result.get('similarity', 0)
                if similarity >= config.SIMILARITY_THRESHOLD:
                    # åˆ›å»ºå…¼å®¹DocumentSourceæ¨¡å‹çš„å¯¹è±¡
                    from core.models import DocumentSource
                    
                    metadata = result.get('metadata', {})
                    source_obj = DocumentSource(
                        title=metadata.get('standard_number', 'æœªçŸ¥æ ‡å‡†'),
                        content=result['content'],
                        source=metadata.get('source_file', 'æœªçŸ¥æ–‡ä»¶'),
                        similarity=similarity,
                        metadata=metadata,
                        file_name=metadata.get('source_file', 'æœªçŸ¥æ–‡ä»¶'),
                        regulation_code=metadata.get('standard_number', ''),
                        section=f"å—{metadata.get('chunk_index', 0)}",
                        similarity_score=similarity
                    )
                    sources.append(source_obj)
        
        if not sources:
            logger.warning(f"çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼ˆé˜ˆå€¼: {config.SIMILARITY_THRESHOLD}ï¼‰ï¼Œä½¿ç”¨æ¨¡å‹é€šç”¨çŸ¥è¯†å›ç­”")
            # å½“çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ£€ç´¢åˆ°ç›¸å…³å†…å®¹æ—¶ï¼Œè®©å¤§æ¨¡å‹åŸºäºè‡ªèº«çŸ¥è¯†ç”Ÿæˆç­”æ¡ˆ
            return llm_service.generate_answer_without_context(request.question)
        
        # è·å–ä¼šè¯å†å²
        session_id = request.session_id or "default"
        history = session_history.get(session_id, [])
        
        # ç”Ÿæˆç­”æ¡ˆ
        response = llm_service.generate_answer(
            question=request.question,
            sources=sources,
            context_history=history
        )
        
        # æ ¹æ®å¤§æ¨¡å‹ç­”æ¡ˆä¸­çš„å¼•ç”¨æŸ¥è¯¢MySQLæ•°æ®åº“è·å–URL
        related_standards = []
        related_regulations = []
        
        if standards_service:
            try:
                logger.info("ğŸ” åˆ†æå¤§æ¨¡å‹ç­”æ¡ˆä¸­çš„å¼•ç”¨...")
                
                # ä»ç­”æ¡ˆä¸­æå–æ ‡å‡†å¼•ç”¨
                answer_text = response.answer
                standard_refs = standards_service.extract_standard_references(answer_text)
                
                if standard_refs:
                    logger.info(f"ğŸ“Š åœ¨ç­”æ¡ˆä¸­å‘ç°æ ‡å‡†å¼•ç”¨: {standard_refs}")
                    for ref in standard_refs:
                        standards = standards_service.search_standards_by_name(ref, 2)
                        related_standards.extend(standards)
                
                # æ£€æŸ¥ç­”æ¡ˆä¸­æ˜¯å¦åŒ…å«æ³•è§„ç›¸å…³å†…å®¹
                regulation_keywords = [
                    'ç®¡ç†åŠæ³•', 'è§„å®š', 'æ¡ä¾‹', 'æš‚è¡ŒåŠæ³•', 'ä½å®…ä¸“é¡¹ç»´ä¿®èµ„é‡‘',
                    'å”®æˆ¿å•ä½', 'å”®æˆ¿æ¬¾', 'å¤šå±‚ä½å®…', 'é«˜å±‚ä½å®…', 'ç¬¬å…«æ¡'
                ]
                
                has_regulation_content = any(keyword in answer_text for keyword in regulation_keywords)
                
                if has_regulation_content:
                    logger.info("ğŸ›ï¸ ç­”æ¡ˆæ¶‰åŠæ³•è§„å†…å®¹ï¼ŒæŸ¥è¯¢regulationsè¡¨...")
                    question_content = request.question
                    combined_content = question_content + " " + answer_text[:500]  # ç»“åˆé—®é¢˜å’Œç­”æ¡ˆå‰500å­—ç¬¦
                    regulations = standards_service.find_regulation_by_content_keywords(combined_content)
                    related_regulations = regulations
                
                # å»é‡æ ‡å‡†
                if related_standards:
                    seen_ids = set()
                    unique_standards = []
                    for standard in related_standards:
                        if standard.id not in seen_ids:
                            seen_ids.add(standard.id)
                            unique_standards.append(standard)
                    related_standards = unique_standards[:3]
                
                # è®°å½•æ‰¾åˆ°çš„èµ„æº
                if related_standards:
                    logger.info(f"âœ… æ‰¾åˆ° {len(related_standards)} ä¸ªç›¸å…³æ ‡å‡†:")
                    for std in related_standards:
                        logger.info(f"  - {std.standard_number}: {std.standard_name}")
                        logger.info(f"    URL: {std.file_url}")
                
                if related_regulations:
                    logger.info(f"âœ… æ‰¾åˆ° {len(related_regulations)} ä¸ªç›¸å…³æ³•è§„:")
                    for reg in related_regulations:
                        logger.info(f"  - {reg.legal_name}")
                        logger.info(f"    URL: {reg.legal_url}")
                    
            except Exception as e:
                logger.error(f"æŸ¥è¯¢MySQLæ•°æ®åº“å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
        
        # æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦çœŸæ­£å›ç­”äº†é—®é¢˜ï¼ˆå†…å®¹ç›¸å…³æ€§æ£€æŸ¥ï¼‰
        # åªæœ‰åœ¨ç¡®å®æ²¡æœ‰æ£€ç´¢åˆ°ä»»ä½•ç›¸å…³å†…å®¹æ—¶æ‰å›é€€
        critical_irrelevant_patterns = [
            "æ ¹æ®æä¾›çš„è§„èŒƒæ–‡æ¡£å†…å®¹ï¼Œæœªæ£€ç´¢åˆ°",
            "æä¾›çš„æ–‡æ¡£ä¸­æ²¡æœ‰æ‰¾åˆ°",
            "æ–‡æ¡£ä¸­æœªåŒ…å«ç›¸å…³ä¿¡æ¯",
            "[ä½¿ç”¨æ ‡å‡†: æ— ]"
        ]
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯å®Œå…¨æ— å…³çš„å›ç­”ï¼ˆæ›´ä¸¥æ ¼çš„æ¡ä»¶ï¼‰
        is_completely_irrelevant = any(pattern in response.answer for pattern in critical_irrelevant_patterns)
        
        # å¦‚æœæ‰¾åˆ°äº†ç›¸å…³çš„æ ‡å‡†æˆ–æ³•è§„ï¼Œå³ä½¿ç­”æ¡ˆä¸­æœ‰"æœªæ‰¾åˆ°"ç­‰è¯æ±‡ï¼Œä¹Ÿä¸åº”è¯¥å›é€€
        has_relevant_resources = (len(related_standards) > 0 or len(related_regulations) > 0)
        
        if is_completely_irrelevant and not has_relevant_resources:
            logger.warning("æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹ä¸é—®é¢˜ä¸å¤Ÿç›¸å…³ï¼Œå›é€€åˆ°æ¨¡å‹çŸ¥è¯†å›ç­”")
            response = llm_service.generate_answer_without_context(request.question)
            
            # ä¸ºå›é€€ç­”æ¡ˆæ·»åŠ ä¼šè¯å†å²
            history.append({"role": "user", "content": request.question})
            history.append({"role": "assistant", "content": response.answer})
            session_history[session_id] = history[-10:]
            response.session_id = session_id
            
            return response
        
        # æå–ç­”æ¡ˆä¸­å®é™…ä½¿ç”¨çš„æ ‡å‡†å¹¶è¿‡æ»¤ç›¸å…³æ ‡å‡†åˆ—è¡¨
        filtered_standards = []
        if related_standards:
            # ä»ç­”æ¡ˆä¸­æå–DeepSeekæ ‡æ³¨çš„ä½¿ç”¨æ ‡å‡†
            used_standards = extract_used_standards_from_answer(response.answer)
            
            if used_standards and "æ— " not in used_standards:
                # æ ¹æ®ç­”æ¡ˆä¸­æ ‡æ³¨çš„æ ‡å‡†è¿‡æ»¤ç›¸å…³æ ‡å‡†åˆ—è¡¨
                for standard in related_standards:
                    standard_num = standard.standard_number.replace(" ", "").replace("-", "")
                    for used_std in used_standards:
                        used_std_clean = used_std.replace(" ", "").replace("-", "")
                        if used_std_clean in standard_num or standard_num in used_std_clean:
                            filtered_standards.append(standard)
                            break
            else:
                # å¦‚æœæ²¡æœ‰æ ‡æ³¨ä½¿ç”¨æ ‡å‡†ï¼Œä½¿ç”¨æ™ºèƒ½è¿‡æ»¤
                filtered_standards = smart_filter_standards(response.answer, related_standards)
            
            # æ·»åŠ è¿‡æ»¤åçš„æ ‡å‡†ä¿¡æ¯
            if filtered_standards:
                standard_info = "\n\nğŸ“‹ **ç›¸å…³å›½å®¶æ ‡å‡†ï¼š**\n"
                for standard in filtered_standards:
                    standard_info += f"â€¢ **{standard.standard_number}**: {standard.standard_name}\n"
                    standard_info += f"  çŠ¶æ€: {standard.status}\n"
                    if standard.file_url:
                        standard_info += f"  ğŸ“„ [æŸ¥çœ‹æ ‡å‡†æ–‡æ¡£]({standard.file_url})\n"
                    standard_info += "\n"
                
                response.answer += standard_info
        
        # æ·»åŠ ç›¸å…³æ³•è§„ä¿¡æ¯
        if related_regulations:
            regulation_info = "\n\nğŸ“‹ **ç›¸å…³æ³•å¾‹æ³•è§„ï¼š**\n"
            for regulation in related_regulations:
                regulation_info += f"â€¢ **{regulation.legal_name}**\n"
                if regulation.legal_url:
                    regulation_info += f"  ğŸ“„ [æŸ¥çœ‹æ³•è§„æ–‡æ¡£]({regulation.legal_url})\n"
                regulation_info += "\n"
            
            response.answer += regulation_info
        
        # æ›´æ–°ä¼šè¯å†å²
        history.append({"role": "user", "content": request.question})
        history.append({"role": "assistant", "content": response.answer})
        session_history[session_id] = history[-10:]  # åªä¿ç•™æœ€è¿‘10è½®å¯¹è¯
        
        response.session_id = session_id
        
        logger.info(f"ç”Ÿæˆç­”æ¡ˆå®Œæˆï¼Œå¯ä¿¡åº¦: {response.confidence_score:.2f}")
        return response
        
    except Exception as e:
        logger.error(f"å¤„ç†é—®é¢˜å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/upload")
async def upload_document(
    file: UploadFile = File(...),
    title: str = Form(...),
    document_type: str = Form("regulation")
):
    """ä¸Šä¼ æ–‡æ¡£åˆ°çŸ¥è¯†åº“"""
    try:
        # æ£€æŸ¥æ–‡ä»¶ç±»å‹
        if not any(file.filename.endswith(ext) for ext in config.SUPPORTED_FILE_TYPES):
            raise HTTPException(
                status_code=400, 
                detail=f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ã€‚æ”¯æŒçš„ç±»å‹: {config.SUPPORTED_FILE_TYPES}"
            )
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        content = await file.read()
        content_str = content.decode('utf-8', errors='ignore')
        
        # åˆ›å»ºæ–‡æ¡£å¯¹è±¡
        document = KnowledgeDocument(
            id=str(uuid.uuid4()),
            title=title,
            content=content_str,
            file_path=file.filename,
            file_type=file.filename.split('.')[-1],
            document_type=document_type,
            upload_time=datetime.now(),
            last_updated=datetime.now()
        )
        
        # æ·»åŠ åˆ°çŸ¥è¯†åº“
        success = kb_manager.add_document(document)
        
        if success:
            return {"message": "æ–‡æ¡£ä¸Šä¼ æˆåŠŸ", "document_id": document.id}
        else:
            raise HTTPException(status_code=500, detail="æ–‡æ¡£ä¸Šä¼ å¤±è´¥")
            
    except Exception as e:
        logger.error(f"æ–‡æ¡£ä¸Šä¼ å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/upload-batch")
async def upload_documents_batch(
    files: List[UploadFile] = File(...),
    chunk_size: int = Form(800),
    chunk_overlap: int = Form(100)
):
    """æ‰¹é‡ä¸Šä¼ æ–‡æ¡£åˆ°çŸ¥è¯†åº“ï¼ˆå¢é‡æ·»åŠ ï¼‰"""
    try:
        if len(files) > 20:  # é™åˆ¶å•æ¬¡ä¸Šä¼ æ–‡ä»¶æ•°é‡
            raise HTTPException(status_code=400, detail="å•æ¬¡æœ€å¤šä¸Šä¼ 20ä¸ªæ–‡ä»¶")
        
        results = []
        total_chunks = 0
        
        for file in files:
            # æ£€æŸ¥æ–‡ä»¶ç±»å‹
            if not any(file.filename.endswith(ext) for ext in config.SUPPORTED_FILE_TYPES):
                results.append({
                    "filename": file.filename,
                    "status": "failed",
                    "error": f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹"
                })
                continue
            
            try:
                # è¯»å–æ–‡ä»¶å†…å®¹
                content = await file.read()
                content_str = content.decode('utf-8', errors='ignore')
                
                # åˆ†å‰²æ–‡æ¡£
                chunks = kb_manager.split_document(content_str, chunk_size, chunk_overlap)
                
                # å‡†å¤‡å…ƒæ•°æ®
                metadatas = []
                for i, chunk in enumerate(chunks):
                    metadata = {
                        "source_file": file.filename,
                        "chunk_index": i,
                        "chunk_count": len(chunks),
                        "document_type": "uploaded",
                        "upload_time": datetime.now().isoformat()
                    }
                    metadatas.append(metadata)
                
                # æ‰¹é‡æ·»åŠ åˆ°çŸ¥è¯†åº“
                doc_ids = kb_manager.add_documents_batch(chunks, metadatas)
                
                results.append({
                    "filename": file.filename,
                    "status": "success",
                    "chunks_added": len(doc_ids),
                    "document_ids": doc_ids[:5]  # åªè¿”å›å‰5ä¸ªID
                })
                
                total_chunks += len(chunks)
                
            except Exception as e:
                results.append({
                    "filename": file.filename,
                    "status": "failed",
                    "error": str(e)
                })
        
        # è·å–æ›´æ–°åçš„çŸ¥è¯†åº“ç»Ÿè®¡
        kb_stats = kb_manager.get_knowledge_base_stats()
        
        return {
            "message": f"æ‰¹é‡ä¸Šä¼ å®Œæˆï¼Œå…±æ·»åŠ  {total_chunks} ä¸ªæ–‡æ¡£å—",
            "total_chunks_added": total_chunks,
            "files_processed": len(files),
            "results": results,
            "knowledge_base_stats": kb_stats
        }
        
    except Exception as e:
        logger.error(f"æ‰¹é‡æ–‡æ¡£ä¸Šä¼ å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/add-text")
async def add_text_to_knowledge_base(
    request: dict
):
    """ç›´æ¥æ·»åŠ æ–‡æœ¬åˆ°çŸ¥è¯†åº“ï¼ˆå¢é‡æ·»åŠ ï¼‰"""
    try:
        text_content = request.get("content", "").strip()
        title = request.get("title", "æ‰‹åŠ¨æ·»åŠ çš„æ–‡æœ¬")
        document_type = request.get("document_type", "manual")
        chunk_size = request.get("chunk_size", 800)
        chunk_overlap = request.get("chunk_overlap", 100)
        
        if not text_content:
            raise HTTPException(status_code=400, detail="æ–‡æœ¬å†…å®¹ä¸èƒ½ä¸ºç©º")
        
        if len(text_content) > 50000:  # é™åˆ¶å•æ¬¡æ·»åŠ çš„æ–‡æœ¬é•¿åº¦
            raise HTTPException(status_code=400, detail="å•æ¬¡æ·»åŠ çš„æ–‡æœ¬é•¿åº¦ä¸èƒ½è¶…è¿‡50000å­—ç¬¦")
        
        # åˆ†å‰²æ–‡æ¡£
        chunks = kb_manager.split_document(text_content, chunk_size, chunk_overlap)
        
        # å‡†å¤‡å…ƒæ•°æ®
        metadatas = []
        for i, chunk in enumerate(chunks):
            metadata = {
                "source_file": title,
                "chunk_index": i,
                "chunk_count": len(chunks),
                "document_type": document_type,
                "add_time": datetime.now().isoformat(),
                "content_length": len(chunk)
            }
            metadatas.append(metadata)
        
        # æ‰¹é‡æ·»åŠ åˆ°çŸ¥è¯†åº“
        doc_ids = kb_manager.add_documents_batch(chunks, metadatas)
        
        # è·å–æ›´æ–°åçš„çŸ¥è¯†åº“ç»Ÿè®¡
        kb_stats = kb_manager.get_knowledge_base_stats()
        
        return {
            "message": f"æˆåŠŸæ·»åŠ æ–‡æœ¬ï¼Œå…±åˆ†å‰²ä¸º {len(chunks)} ä¸ªæ–‡æ¡£å—",
            "title": title,
            "chunks_added": len(chunks),
            "document_ids": doc_ids,
            "knowledge_base_stats": kb_stats
        }
        
    except Exception as e:
        logger.error(f"æ·»åŠ æ–‡æœ¬åˆ°çŸ¥è¯†åº“å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/remove-documents")
async def remove_documents_by_source(
    source_file: str
):
    """æ ¹æ®æ¥æºæ–‡ä»¶åˆ é™¤æ–‡æ¡£ï¼ˆç”¨äºæ›´æ–°æ–‡æ¡£ï¼‰"""
    try:
        # è¿™ä¸ªåŠŸèƒ½éœ€è¦åœ¨BigModelKnowledgeBaseä¸­å®ç°
        # ç›®å‰ChromaDBæ”¯æŒæ ¹æ®metadataè¿‡æ»¤åˆ é™¤
        removed_count = kb_manager.remove_documents_by_source(source_file)
        
        kb_stats = kb_manager.get_knowledge_base_stats()
        
        return {
            "message": f"æˆåŠŸåˆ é™¤æ¥æºä¸º '{source_file}' çš„æ–‡æ¡£",
            "removed_count": removed_count,
            "knowledge_base_stats": kb_stats
        }
        
    except Exception as e:
        logger.error(f"åˆ é™¤æ–‡æ¡£å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/status", response_model=SystemStatus)
async def get_system_status():
    """è·å–ç³»ç»ŸçŠ¶æ€"""
    try:
        stats = kb_manager.get_knowledge_base_stats()
        
        return SystemStatus(
            status="æ­£å¸¸è¿è¡Œ",
            knowledge_base_stats=stats,
            llm_service_status="æ­£å¸¸",
            uptime="è¿è¡Œä¸­"
        )
        
    except Exception as e:
        logger.error(f"è·å–ç³»ç»ŸçŠ¶æ€å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/search")
async def search_knowledge_base(query: str, top_k: int = 5):
    """æœç´¢å½“å‰çŸ¥è¯†åº“"""
    try:
        sources_result = kb_manager.search(query, n_results=top_k)
        
        results = []
        if sources_result and "results" in sources_result:
            for result in sources_result["results"]:
                results.append({
                    "content": result['content'][:200] + "..." if len(result['content']) > 200 else result['content'],
                    "file_name": result.get('metadata', {}).get('source_file', 'æœªçŸ¥æ–‡ä»¶'),
                    "standard_number": result.get('metadata', {}).get('standard_number', ''),
                    "document_type": result.get('metadata', {}).get('document_type', ''),
                    "similarity_score": result.get('similarity', 0)
                })
        
        return {
            "query": query,
            "collection": DEFAULT_COLLECTION,
            "collection_name": KNOWLEDGE_BASES[DEFAULT_COLLECTION],
            "results": results
        }
        
    except Exception as e:
        logger.error(f"çŸ¥è¯†åº“æœç´¢å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/knowledge-bases")
async def get_knowledge_bases():
    """è·å–å¯ç”¨çš„çŸ¥è¯†åº“åˆ—è¡¨"""
    try:
        # æ£€æŸ¥æ¯ä¸ªçŸ¥è¯†åº“çš„çŠ¶æ€
        kb_status = {}
        for kb_id, kb_name in KNOWLEDGE_BASES.items():
            try:
                # ä¸´æ—¶åˆ›å»ºçŸ¥è¯†åº“ç®¡ç†å™¨æ£€æŸ¥çŠ¶æ€
                temp_kb = KnowledgeBaseManager(
                    api_key=config.bigmodel_api_key,
                    collection_name=kb_id
                )
                info = temp_kb.get_collection_info()
                kb_status[kb_id] = {
                    "name": kb_name,
                    "status": "available",
                    "document_count": info.get('count', 0),
                    "is_current": kb_id == DEFAULT_COLLECTION
                }
            except Exception as e:
                kb_status[kb_id] = {
                    "name": kb_name,
                    "status": "not_available",
                    "document_count": 0,
                    "is_current": kb_id == DEFAULT_COLLECTION,
                    "error": str(e)
                }
        
        return {
            "current_collection": DEFAULT_COLLECTION,
            "knowledge_bases": kb_status
        }
        
    except Exception as e:
        logger.error(f"è·å–çŸ¥è¯†åº“åˆ—è¡¨å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/switch-knowledge-base")
async def switch_knowledge_base(request: dict):
    """åˆ‡æ¢çŸ¥è¯†åº“"""
    global kb_manager, DEFAULT_COLLECTION
    
    # å¤„ç†è¯·æ±‚å‚æ•°
    if isinstance(request, str):
        collection_name = request
    else:
        collection_name = request.get("collection_name") or request
    
    if collection_name not in KNOWLEDGE_BASES:
        raise HTTPException(
            status_code=400, 
            detail=f"æœªçŸ¥çš„çŸ¥è¯†åº“: {collection_name}. å¯ç”¨çš„çŸ¥è¯†åº“: {list(KNOWLEDGE_BASES.keys())}"
        )
    
    try:
        # åˆ›å»ºæ–°çš„çŸ¥è¯†åº“ç®¡ç†å™¨
        new_kb_manager = KnowledgeBaseManager(
            api_key=config.bigmodel_api_key,
            collection_name=collection_name
        )
        
        # æµ‹è¯•æ–°çŸ¥è¯†åº“æ˜¯å¦å¯ç”¨
        info = new_kb_manager.get_collection_info()
        
        # åˆ‡æ¢æˆåŠŸ
        kb_manager = new_kb_manager
        DEFAULT_COLLECTION = collection_name
        
        logger.info(f"æˆåŠŸåˆ‡æ¢åˆ°çŸ¥è¯†åº“: {collection_name} ({KNOWLEDGE_BASES[collection_name]})")
        
        return {
            "message": f"å·²åˆ‡æ¢åˆ° {KNOWLEDGE_BASES[collection_name]}",
            "collection": collection_name,
            "document_count": info.get('count', 0),
            "embedding_model": info.get('embedding_model', ''),
            "embedding_dimension": info.get('embedding_dimension', 0)
        }
        
    except Exception as e:
        logger.error(f"åˆ‡æ¢çŸ¥è¯†åº“å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"åˆ‡æ¢çŸ¥è¯†åº“å¤±è´¥: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "main:app",
        host=config.HOST,
        port=config.PORT,
        reload=config.DEBUG,
        log_level="info"
    ) 